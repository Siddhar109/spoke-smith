# ğŸ™ï¸ SpokeSmith

**AI-powered media training coach** for high-stakes spokesperson moments (interviews, crisis comms, product launches).

## ğŸ”— Demo

- Product live demo: https://spoke-smith-87008435117.us-central1.run.app/
- Demo video: https://www.loom.com/share/4c660e0fe125402a9a045dbfdf5b1518

---

## What problem are we solving?

In front of a camera, people juggle **content + delivery + composure**. In high-stakes moments, even strong operators can:
- Ramble instead of landing a crisp message
- Speak too fast / too flat
- Lose framing/lighting and look unprepared
- Miss opportunities to bridge back to the headline

Traditional media training helps, but itâ€™s expensive and not always available â€œon demandâ€.

---

## What is SpokeSmith?

SpokeSmith is a practice environment where you can:
- Run a live voice session with an AI journalist or coach (OpenAI Realtime)
- See a live metrics HUD (pace, fillers, prosody)
- Get short, structured nudges (typed tool calls) while you practice
- Generate scenarios from company context (Responses API + web search)
- Export a timestamped transcript after the session (Whisper)

---

## âœ¨ Key features

### ğŸ§ Voice-first practice (Realtime API via WebRTC)
- Low-latency speech-to-speech session with `gpt-4o-realtime-*`
- Two modes: **journalist** (interview) or **coach** (feedback + drills)
- Nudges arrive as a typed tool call: `nudge(text, severity, reason)`

### ğŸ“Š Live metrics HUD (local)
- Pace (WPM), filler rate, prosody variance (expressiveness)
- Lightweight â€œmomentumâ€ score for at-a-glance feedback

### ğŸ‘¤ Presence signals (local, privacy-first)
- On-device MediaPipe face tracking: `face_present`, `framing`, `lighting`
- Optional: use Responses API to rewrite/verify face nudges (off by default)

### ğŸ­ Company-aware scenarios (Responses API)
- Company brief summarization using the `web_search` tool
- Scenario generation with **Structured Outputs** (strict JSON schema)

### ğŸ§¾ Post-session transcript (Whisper)
- Word-level timestamps (`verbose_json`) for timeline scrubbing
- (Planned) deeper post-session analysis and rewrites

---

## ğŸ§  How it works (end-to-end)

```
1) (Optional) Add company context
   Browser â”€â”€â–¶ POST /api/company_brief â”€â”€â–¶ OpenAI Responses + web_search â”€â”€â–¶ brief JSON

2) Generate a scenario
   Browser â”€â”€â–¶ POST /api/scenario/generate â”€â”€â–¶ OpenAI Responses (strict json_schema) â”€â”€â–¶ scenario JSON

3) Start a live session (voice-first)
   Browser â”€â”€â–¶ POST /api/realtime/token â”€â”€â–¶ FastAPI â”€â”€â–¶ OpenAI /v1/realtime/sessions
                                 â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ephemeral client_secret

4) Practice (no audio proxying through our servers)
   Browser â”€â”€(WebRTC, Bearer: client_secret)â”€â”€â–¶ OpenAI /v1/realtime?model=...
   Browser â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI audio replies + tool calls (nudges) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI Realtime

5) End session â†’ transcript with timestamps
   Browser â”€â”€â–¶ POST /api/sessions (audio blob) â”€â”€â–¶ whisper-1 (verbose_json) â”€â”€â–¶ words[] timestamps

6) (Optional) Face nudges: local thresholds + optional LLM phrasing/verify
   Browser â”€â”€â–¶ POST /api/face/nudge/phrase (json_schema) â”€â”€â–¶ short rewritten nudge text
   Browser â”€â”€â–¶ POST /api/face/nudge/verify (json_schema + cropped keyframe) â”€â”€â–¶ verified/abstain
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       same-origin `/api/*` proxy       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Next.js UI           â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚        FastAPI API        â”‚
â”‚ - mic/cam capture             â”‚                                         â”‚ - /api/realtime/token     â”‚
â”‚ - local voice metrics (HUD)   â”‚                                         â”‚ - /api/company_brief      â”‚
â”‚ - local face metrics (WASM)   â”‚                                         â”‚ - /api/scenario/generate  â”‚
â”‚ - session store (Zustand)     â”‚                                         â”‚ - /api/sessions           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚ - /api/face/nudge/*       â”‚
                â”‚                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ WebRTC (ephemeral token; live audio never hits our backend)          â”‚
                â–¼                                                                       â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    OpenAI Realtime    â”‚                                            â”‚ OpenAI Responses + Audio â”‚
     â”‚  - speech â†” speech    â”‚                                            â”‚ - json_schema outputs    â”‚
     â”‚  - input transcriptionâ”‚                                            â”‚ - web_search tool        â”‚
     â”‚  - tool calls (nudges)â”‚                                            â”‚ - whisper-1 transcriptionâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ OpenAI services (and why)

### Realtime API (WebRTC) â€” voice coaching + interview mode
- **Default model**: `OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview-2024-12-17`
- **Realtime transcription**: `OPENAI_TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe`
- **Why**: low-latency speech-to-speech makes practice feel like a real conversation.

### Responses API (Structured Outputs) â€” UI-safe generation
We use `text.format.type=json_schema` so the UI doesnâ€™t parse fragile JSON:
- **Company brief**: `OPENAI_COMPANY_BRIEF_MODEL` (default: `gpt-5-mini`) with `tools: [{ "type": "web_search" }]`
- **Scenario generation**: `OPENAI_SCENARIO_MODEL` (default: `gpt-5-mini`) with strict JSON schema
- **Face nudge phrasing/verify (optional)**: `OPENAI_FACE_PHRASE_MODEL` (default: `gpt-4o-mini`) + optional `OPENAI_FACE_VERIFY_MODEL` (image input supported)

### Audio Transcriptions â€” post-session timestamps
- **Model**: `whisper-1` with `verbose_json` + `timestamp_granularities: ["word"]`

---

## ğŸ¯ Nudge design

### Voice nudges (Realtime tool calls)
- The realtime model emits `nudge()` tool calls in **coach** mode based on prompt guardrails (keep it rare, keep it short).
- Nudges are structured: `text` (<= ~10 words), `severity` (`gentle|firm|urgent`), `reason` (e.g. `pace`, `filler`, `risk`).

### Face nudges (deterministic + optional LLM help)
- Face signals are computed on-device; issues must persist ~2s before nudging.
- A session-level cooldown reduces spam (default ~12s between face nudges).
- If enabled:
  - **Phase B (text-only)**: rewrite the nudge via `POST /api/face/nudge/phrase`
  - **Phase C (optional)**: verify with a **cropped, rate-limited keyframe** via `POST /api/face/nudge/verify`

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| **Frontend** | Next.js 15, React 18, TypeScript |
| **Styling** | Tailwind CSS, Framer Motion |
| **State** | Zustand (with persistence) |
| **Face Detection** | MediaPipe FaceLandmarker (WASM) |
| **Audio Analysis** | Web Audio API (browser-native) |
| **Backend** | FastAPI, Python 3.11+ |
| **AI Models** | OpenAI (Realtime, Responses, Whisper) |
| **Real-time Comms** | WebRTC |

---

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+
- Python 3.11+
- OpenAI API key

### 1. Clone the repository

```bash
git clone https://github.com/Siddhar109/spoke-smith
cd spoke-smith
```

### 2. Set up environment variables

```bash
cp .env.example .env
```

Edit `.env`:

```env
OPENAI_API_KEY=sk-your-key-here
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview-2024-12-17
OPENAI_TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe

# Used by the Next.js `/api/*` proxy route (server-side). Either works:
NEXT_PUBLIC_API_URL=http://localhost:8000
# BACKEND_API_URL=http://localhost:8000
```

### 3. Install and run the backend

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

### 4. Install and run the frontend

```bash
npm install
npm run dev
```

### 5. Open in browser

Navigate to `http://localhost:3000` and start practicing!

---

## ğŸ”§ Configuration

### Backend env vars (FastAPI)

- `OPENAI_API_KEY` (required)
- `OPENAI_REALTIME_MODEL` (optional)
- `OPENAI_TRANSCRIPTION_MODEL` (optional)
- `OPENAI_COMPANY_BRIEF_MODEL`, `OPENAI_COMPANY_BRIEF_MAX_OUTPUT_TOKENS`, `OPENAI_COMPANY_BRIEF_LIST_LIMIT` (optional)
- `OPENAI_SCENARIO_MODEL`, `OPENAI_SCENARIO_MAX_OUTPUT_TOKENS` (optional)
- `OPENAI_FACE_PHRASE_MODEL`, `OPENAI_FACE_VERIFY_MODEL`, `FACE_NUDGE_DEFAULT_COOLDOWN_MS` (optional)
- `KAWKAI_KEEP_SESSION_AUDIO` (optional; defaults to deleting uploads after transcription)
- `CORS_ALLOW_ORIGINS`, `CORS_ALLOW_ORIGIN_REGEX` (optional; mostly for direct-calling backend)

### Frontend env vars (Next.js)

- `NEXT_PUBLIC_API_URL` or `BACKEND_API_URL` (where the backend lives; used by the server-side `/api/*` proxy)
- `NEXT_PUBLIC_FACE_PHRASE_MODEL_ENABLED` (optional; enables LLM phrasing for face nudges)

---

## ğŸ§© API surface (backend)

- `POST /api/realtime/token` â†’ ephemeral Realtime client secret
- `POST /api/company_brief` â†’ company brief summary (structured JSON)
- `POST /api/scenario/generate` â†’ one generated scenario (structured JSON)
- `POST /api/sessions` â†’ upload audio + receive transcript + `word_timings`
- `GET /api/sessions/{session_id}/transcript` â†’ fetch stored transcript (if present)
- `POST /api/face/nudge/phrase` â†’ short, rephrased face nudge (optional feature)
- `POST /api/face/nudge/verify` â†’ keyframe-based verification (optional feature)
- `GET /health` â†’ healthcheck
- `GET /docs` â†’ Swagger UI

---

## ğŸ“¦ Deployment

- Cloud Run (2 services): see `DEPLOY_CLOUD_RUN.md`

---

## ğŸ“š More docs

- `implementation.md`
- `IMPLEMENTATION_PLAN_CONTEXT_ROUTING.md`
- `UPGRADE_PLAN.md`

---

## ğŸ“ Project Structure

```
spoke-smith/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                    # Next.js pages
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Landing page
â”‚   â”‚   â””â”€â”€ session/            # Main coaching interface
â”‚   â”‚
â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”‚   â”œâ”€â”€ LiveMeters.tsx      # Real-time metric visualizations
â”‚   â”‚   â”œâ”€â”€ LiveNudge.tsx       # Floating nudge overlay
â”‚   â”‚   â”œâ”€â”€ VideoPreview.tsx    # Webcam feed with face tracking
â”‚   â”‚   â”œâ”€â”€ ScenarioSelector.tsx # Scenario browser
â”‚   â”‚   â””â”€â”€ Timeline.tsx        # Post-session playback
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/                  # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ useRealtimeCoach.ts # WebRTC + Realtime API
â”‚   â”‚   â”œâ”€â”€ useFaceCoach.ts     # MediaPipe face detection
â”‚   â”‚   â”œâ”€â”€ useMediaCapture.ts  # Mic + cam management
â”‚   â”‚   â””â”€â”€ useAudioRecorder.ts # Audio blob recording
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ analysis/           # Metric calculations
â”‚   â”‚   â”‚   â”œâ”€â”€ voiceMetrics.ts # WPM, filler detection
â”‚   â”‚   â”‚   â””â”€â”€ prosody.ts      # Pitch variance analysis
â”‚   â”‚   â””â”€â”€ scenarios/          # Scenario definitions
â”‚   â”‚
â”‚   â””â”€â”€ stores/
â”‚       â””â”€â”€ sessionStore.ts     # Zustand state management
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/                    # FastAPI routers
â”‚   â”‚   â”œâ”€â”€ realtime.py         # Ephemeral token generation
â”‚   â”‚   â”œâ”€â”€ sessions.py         # Audio upload + transcription
â”‚   â”‚   â”œâ”€â”€ face_nudge.py       # Face nudge phrase/verify
â”‚   â”‚   â”œâ”€â”€ company_brief.py    # Company context summarizer
â”‚   â”‚   â””â”€â”€ scenario.py         # Custom scenario generator
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                # AI prompt templates
â”‚   â”‚   â”œâ”€â”€ coach_system.py     # Coaching mode prompt
â”‚   â”‚   â”œâ”€â”€ journalist_system.py # Interview mode prompt
â”‚   â”‚   â””â”€â”€ nudge_tools.py      # Tool definitions
â”‚   â”‚
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ transcription.py    # Whisper integration
â”‚
â””â”€â”€ public/
    â””â”€â”€ models/                 # MediaPipe model files
```

---

## ğŸ”’ Privacy & Data

- **Face analysis is local-only** â€” MediaPipe runs in browser WASM.
- **No face images uploaded by default** â€” keyframe verification is opt-in, cropped, and rate-limited.
- **Session audio is deleted by default** after `whisper-1` transcription (set `KAWKAI_KEEP_SESSION_AUDIO=true` to keep uploads).
- **Responses API calls use `store: false`** in this repo; OpenAI policies still apply to any data you send.

---

## ğŸ† Built for OpenAI Hackathon

This repo is intentionally â€œAPI-forwardâ€: Realtime for the session, Responses for structured generation (plus web search), and Whisper for word-level timestamps â€” stitched together with a UX that tries to feel like a real coach.

---

<p align="center">
  <b>Stop losing deals, damaging reputations, and missing opportunities.</b><br>
  <i>Train like the pros. Sound like a leader.</i>
</p>
